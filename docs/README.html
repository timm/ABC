<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Tim Menzies timm@ieee.org" />
  <title>Easier AI via Data Minign Algirithms Using/Used-by Optimizers</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {  background-color: #f8f8f8; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ef2929; } /* Alert */
    code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #204a87; } /* Attribute */
    code span.bn { color: #0000cf; } /* BaseN */
    code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4e9a06; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #8f5902; font-style: italic; } /* Comment */
    code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
    code span.dt { color: #204a87; } /* DataType */
    code span.dv { color: #0000cf; } /* DecVal */
    code span.er { color: #a40000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #0000cf; } /* Float */
    code span.fu { color: #204a87; font-weight: bold; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
    code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
    code span.ot { color: #8f5902; } /* Other */
    code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
    code span.sc { color: #ce5c00; font-weight: bold; } /* SpecialChar */
    code span.ss { color: #4e9a06; } /* SpecialString */
    code span.st { color: #4e9a06; } /* String */
    code span.va { color: #000000; } /* Variable */
    code span.vs { color: #4e9a06; } /* VerbatimString */
    code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
   pre {font-size: x-small;} </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Easier AI via Data Minign Algirithms Using/Used-by
Optimizers</h1>
<p class="author">Tim Menzies<br>timm@ieee.org</p>
<p class="date">July, 2025</p>
</header>
<table style="width:76%;">
<colgroup>
<col style="width: 76%" />
</colgroup>
<tbody>
<tr>
<td><em>“The best thing to do with most data is throw it away.”</em>
–me</td>
</tr>
<tr>
<td><em>“YAGNI”</em> (you aren’t gonna need it) –Kent Beck</td>
</tr>
<tr>
<td><em>“Less, but better.”</em> –Dieter Rams</td>
</tr>
<tr>
<td><em>“Subtract.”</em> –Leidy Klotz</td>
</tr>
</tbody>
</table>
<h2 id="introduction">Introduction</h2>
<p>Have we made AI too complicated? Are all AI tasks generative tasks
that require expensive licenses of large models feed by trillions of
variables, requiring centruies of CPU? Or are their simpler methods,
worthy of much active research? Clearly, not everything can be made
simpler. Generative tasks need a large knowldge based on conditional
probabilities in order to knw what to gnerate next. But not all tasks
are generative tasks. And of those non-generative tasks, are there some
that are very simple to code, easy to train, and fast to run?</p>
<p>To explore this issue, this paper is about a tiny code based that
implements explanation of active learning for multi-objective optimzaion
problems. Having explore this area for over two decades, I can assert
that there are many complex ways to adress explanaton, active learning,
and optimization. Here, we step away from thos methods, seeking very
simple approaches.</p>
<p>This work began with a question for a graduate student in an AI
class:</p>
<blockquote>
<p>Why do our models need so much data?</p>
</blockquote>
<p>“Maybe they don’t,” I replied. “Maybe we just have not learned how to
ask the right questions yet.”</p>
<p>My claim led to a dare which led to a prototype. Given a database of
past examples sorted in “best” and “rest” EZR is a Bayesian contrast set
learner that prefers new examples if they are more likely to be best
than rest. The most preferred example then updates “best” and “rest” and
the cycle repeats. At runtime, EZR avoids data that is noisey (i.e. is
clearly not “best” or “rest”) and superfluous (i.e. is not not relevant
for “better” behavior). In this way it ignores most of the data and
builds its models using just a few dozens samples. A regression tree
learner uses those exampeles to build a tiny tree that not only explains
how to acheive good results, but also what to do to imroe those
results.</p>
<p>EZR has been tested on dozens problems from recent SE research papers
that :</p>
<ul>
<li>Try to build design that minimize cost, defects, development time
and mazimize functionality;</li>
<li>Select the magic control parameters that data miners use to guide
thei learning (e.g. how many trees in a random forest?);</li>
<li>Configure cloud or database software</li>
<li>Predict the health of open source software rojects in one eyar’s
time;</li>
<li>Optimize the management of software proects</li>
<li>And others.</li>
</ul>
<p>We also look at other tasks:</p>
<ul>
<li>Select teams of good football players;</li>
<li>Retrain employees for a longer period of time;</li>
<li>Reduce student sdtop our fro school</li>
<li>Balance a pole on a cart</li>
<li>Decide which house to biy or loan applicant to approave</li>
<li>Predict life expectancy or predict patters or infectios diseases or
patience re-admittance.</li>
<li>Design cars, select wine</li>
<li>Determine what advertising campaigns camight be successufl</li>
<li>And others.</li>
</ul>
<p>For these tasks, EZR was found to be effective at selecting optimal
solutions. Its recommendations are completitove with alternate
technologies Gaussian process models, reinforement learning, iterated
racing etc etc. It also runs two to four orders of magnitude faster.</p>
<p>Lastly, it has a tiny code base. This paper includes a complete
source code listing for EZR. This code makes no use of complex Python
packages like Pandas or scikit-learn. Yet in under th300 lines it
implements explanation of active learning for multi-objective optimzaion
problems. Such a small code base makes it useful material for teaching
the basics of AI and SE scripting. We have also found it a productive
workbench within which we can explore state-of-the-art research.</p>
<p>This experience wuth EZR has shifted our thinking. The sucess of EZR
was not luck- it points to something deeper: an approach to software
engineering and AI where, often, a few well-chosen examples outperform
brute-force data collection. For centuries, researchers have told us
that that key variable selection, and strategic sampling can solve
complex problems efficiently (see Table 1).</p>
<p>Yet despite this legacy of elegant minimalism, today’s trends often
favor the opposite: ever-larger models, more data, and growing
complexity. There are many reasons for this[^subtract], but a major one
is inertia: we tend to add rather than subtract, mistaking bloat for
progress. Complex products impress the marketplace and so can drive
sales (even if, later, that also drives up maintainance costs).</p>
<table>
<colgroup>
<col style="width: 6%" />
<col style="width: 17%" />
<col style="width: 15%" />
<col style="width: 60%" />
</colgroup>
<thead>
<tr>
<th>Year</th>
<th>What</th>
<th>Who / System</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>1902</td>
<td>PCA</td>
<td>Pearson</td>
<td>Larger matrices can be projected down to a few components.</td>
</tr>
<tr>
<td>1960s</td>
<td>Narrows</td>
<td>Amarel <a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a></td>
<td>Search can be guided by tiny sets of key variable settings.</td>
</tr>
<tr>
<td>1974</td>
<td>Prototypes</td>
<td>Chang <a href="#fn2" class="footnote-ref" id="fnref2"
role="doc-noteref"><sup>2</sup></a></td>
<td>Nearest neighbor reasoning is quicker after discarding 90% of the
data and keeping only the best exemplars.</td>
</tr>
<tr>
<td>1980s</td>
<td>ATMS</td>
<td>de Kleer</td>
<td>Diagnoses is quicker when it focus only on the core assumptions that
do not depend on other assumptions.</td>
</tr>
<tr>
<td>1984</td>
<td>Distance-Preseration</td>
<td>Johnson and Lindenstrauss <a href="#fn3" class="footnote-ref"
id="fnref3" role="doc-noteref"><sup>3</sup></a></td>
<td>High-dimensional data can be embedded in low dimensions while
preserving pairwise distances.</td>
</tr>
<tr>
<td>1996</td>
<td>ISAMP</td>
<td>Crawford &amp; Baker <a href="#fn4" class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a></td>
<td>Best solutions lie is small parts of search space. Fast random tries
and frequent retries is fast way to explore that space.</td>
</tr>
<tr>
<td>1997</td>
<td>Feature Subset Selection</td>
<td>John &amp; Kohavi <a href="#fn5" class="footnote-ref" id="fnref5"
role="doc-noteref"><sup>5</sup></a></td>
<td>Up to 80% of features can be ignored without hurting accuracy.</td>
</tr>
<tr>
<td>2002</td>
<td>Backdoors</td>
<td>Williams et al. <a href="#fn6" class="footnote-ref" id="fnref6"
role="doc-noteref"><sup>6</sup></a></td>
<td>Setting a few variables beforehand reduces exponential runtimes to
polynomial.</td>
</tr>
<tr>
<td>2005</td>
<td>Semi-Supervised Learning</td>
<td>Zhou et al. <a href="#fn7" class="footnote-ref" id="fnref7"
role="doc-noteref"><sup>7</sup></a></td>
<td>Data often lies on low-dimensional manifolds inside high-dimensional
spaces.</td>
</tr>
<tr>
<td>2008</td>
<td>Exemplars</td>
<td>Menzies <a href="#fn8" class="footnote-ref" id="fnref8"
role="doc-noteref"><sup>8</sup></a></td>
<td>Small samples can summarize and model large datasets.</td>
</tr>
<tr>
<td>2009</td>
<td>Active Learning</td>
<td>Settles <a href="#fn9" class="footnote-ref" id="fnref9"
role="doc-noteref"><sup>9</sup></a></td>
<td>Best results come from learners reflecting on their own models to
select their own training examples.</td>
</tr>
<tr>
<td>2005–20</td>
<td>Key Vars in SE</td>
<td>Menzies et al.</td>
<td>Dozens of SE models are controlled by just a few parameters.</td>
</tr>
<tr>
<td>2010+</td>
<td>Surrogate Models</td>
<td>Various</td>
<td>Optimizers can be approximated from small training sets.</td>
</tr>
<tr>
<td>2020s</td>
<td>Model Distillation</td>
<td>Various</td>
<td>Large AI models can be reduced in size by orders of magnitude, with
little performance loss.</td>
</tr>
</tbody>
</table>
<p>Using this code and data, we ask five questions:</p>
<ul>
<li><p><strong>RQ1: Is EZR simple?</strong><br />
Is our code base compact and readable and suitabke for teaching and
tinkering?</p></li>
<li><p><strong>RQ2: Is EZR fast?</strong><br />
Can our code complete tasks in milliseconds rather than hours?</p></li>
<li><p><strong>RQ3: Is EZR effective?</strong><br />
Can our achieve strong results after seeing only a few
examples?</p></li>
<li><p><strong>RQ4: Is EZR insightful?</strong><br />
Does our code support explainability, critique, and
understanding?</p></li>
<li><p><strong>RQ5: Is EZR general?</strong><br />
Does our code apply across various tasks?</p></li>
</ul>
<p>This last question deserves extra attention. EZR is not a solution to
everythng. Some tasks are inherently complex and need inherently compex
solutions. or example, this document was written with extensive
editorial support by a large language model (big AI was used to
sumamrize verbose sections into shorter and simpler segments). In RQ5,
we describe the “BINGO” test which is a way to peek at data in order to
decide they you need something more than just EZR.</p>
<p>All the code and data used here can be accessed as fllows</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>mkdir demo<span class="op">;</span> cd demo</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>git clone https:<span class="op">//</span>github.com<span class="op">/</span>timm<span class="op">/</span>moot <span class="co"># &lt;== data</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>git clone https:<span class="op">//</span>github.com<span class="op">/</span>timm<span class="op">/</span>abc  <span class="co"># &lt;== code</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>cd abc</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>python3 <span class="op">-</span>B abc.py <span class="op">-</span>f ..<span class="op">/</span>moot<span class="op">/</span>optimize<span class="op">/</span>config<span class="op">/</span>SS<span class="op">-</span>M.csv <span class="co"># &lt;== test case </span></span></code></pre></div>
<h2 id="a-quick-example">A Quick Example</h2>
<p>Just to give this work some context, here’s a concrete case.</p>
<p>Say we want to configure a database to reduce energy use, runtime,
and CPU load. The system exposes dozens of tuning knobs—storage,
logging, locking, encryption, and more. Understanding how each setting
impacts performance is daunting.</p>
<p>When manual reasoning fails, we can ask AI to help. Imagine we have a
log of 800+ configurations, each showing the settings to dozens of
control settings, and their measured effects. Some settings lead to
excellent results:</p>
<pre><code>choices                     Effects
----------------            -----------------------
control settings →            Energy-, time-,  cpu-
0,0,0,0,1,0,...               6.6,    248.4,   2.1   ← best
1,1,0,1,1,1,...              16.8,    518.6,  14.1   ← rest
...</code></pre>
<p>Any number of AI tools could learn what separates “best” from “rest.”
But here’s the challenge: <strong>labeling</strong> each configuration
(e.g., by running all the benchmarks for all possible configurations) is
expensive. So the EZR challenge is how to learn an effective model with
minimal effort?</p>
<p>That’s where EZR comes in. We call it that since it uses a minimalist
A–B–C strategy:</p>
<ul>
<li><p><strong>A=Any</strong>; i.e. “ask anything”. Randomly label a few
examples (say, <em>A = 4</em>) to seed the process.</p></li>
<li><p><strong>B=Build</strong>; i.e. build a model** In this phase, we
build contrastive models for “best” and “rest,” then label up to, say
<em>B = 24</em> more rows by picking the unlabelled row that maximizes
the score <em>b/r</em> (where <code>b</code> and <code>r</code> are
likelihoods that a row belongs to the “best” and “rest”
models).</p></li>
<li><p><strong>C=Check</strong>; i.e. check the model. Apply the learned
model to unlabeled data and to select a small set (e.g., <em>C=5</em>)
of the most promising rows. After labelling those rows, return the best
one.</p></li>
</ul>
<p>In this task, after labeling just 24 out of 800 rows (∼3%), EZR
constructs a binary regression tree from those 24 examples. In that
tree, left and right branches go best and worse examples. The left-most
branch of that tree is shown here (and to get to any line in this tree,
all the things abve it have to be true).</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> crypt_blowfish <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="op">|</span>  <span class="cf">if</span> memory_tables <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="op">|</span>  <span class="op">|</span>  <span class="cf">if</span> detailed_logging <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="op">|</span>  <span class="op">|</span>  <span class="op">|</span>  <span class="cf">if</span> no_write_delay <span class="op">==</span> <span class="dv">0</span><span class="op">;</span> <span class="op">&lt;==</span> win<span class="op">=</span><span class="dv">98</span><span class="op">%</span></span></code></pre></div>
<p>These four conditions select rows that vey clone (98%) to optimial
Note that this branch only mentions four options, and two of those are
all about what to turn off. That is to say, even though this databased
has dozens of configuration options, there are two bad things to avoid
and only two most important thing to enable (<em>memory_tables</em> and
<em>detailed_logging</em>).</p>
<p>Of course, if you ever show a result like this to a subject matter
expert, they will push back. For example, they might demand to know what
happens when <code>crypt_blowfish</code> is enabled. Blowfish in
password hashing scheme. It makes password protection slower but it also
increases the computational effort required for attackers to brute-force
attack the database’s security. The full tree generted by EZR shows what
happens this feature is enabled. (see the last two lines). Note all the
negative “wins” which is to say, if your goals are fast runtimes, do not
<code>crypt_blowfish</code>.</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a> <span class="co">#rows  win</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="dv">17</span>   <span class="dv">68</span>    <span class="cf">if</span> crypt_blowfish <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>     <span class="dv">7</span>   <span class="dv">94</span>    <span class="op">|</span>  <span class="cf">if</span> memory_tables <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>     <span class="dv">5</span>   <span class="dv">97</span>    <span class="op">|</span>  <span class="op">|</span>  <span class="cf">if</span> detailed_logging <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>     <span class="dv">4</span>   <span class="dv">98</span>    <span class="op">|</span>  <span class="op">|</span>  <span class="op">|</span>  <span class="cf">if</span> no_write_delay <span class="op">==</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="dv">10</span>   <span class="dv">49</span>    <span class="op">|</span>  <span class="cf">if</span> memory_tables <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>     <span class="dv">7</span>   <span class="dv">51</span>    <span class="op">|</span>  <span class="op">|</span>  <span class="cf">if</span> encryption <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>     <span class="dv">5</span>   <span class="dv">50</span>    <span class="op">|</span>  <span class="op">|</span>  <span class="op">|</span>  <span class="cf">if</span> no_write_delay <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>     <span class="dv">4</span>   <span class="dv">50</span>    <span class="op">|</span>  <span class="op">|</span>  <span class="op">|</span>  <span class="op">|</span>  <span class="cf">if</span> txc_mvlocks <span class="op">==</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>     <span class="dv">7</span> <span class="op">-</span><span class="dv">165</span>    <span class="cf">if</span> crypt_blowfish <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>     <span class="dv">4</span>  <span class="op">-</span><span class="dv">51</span>    <span class="op">|</span>  <span class="cf">if</span> memory_tables <span class="op">==</span> <span class="dv">1</span><span class="op">;</span></span></code></pre></div>
<p>(Aside: of course if security is an important goal then (a) add a
“security+” column to the training data shown above; (b) re-un EZR; (c)
check what are the mpracts of that additional goal.)</p>
<p>EZR shows that with the right strategy, a handful of examples (in
this case, 24) can uncover nearly all the signal. All of this took just
a few dozen queries—and a few hundred lines of code. It’s a striking
illustration of the Pareto principle: <strong>most of the value often
comes from just a small fraction of the effort</strong>.</p>
<h2 id="discussion">Discussion</h2>
<p>Before looking at the code, we address the questions often asked
about the above exampel?</p>
<h3 id="why-not-label-everything">Why not label everything?</h3>
<p>The above code assumed we can only label 24 our of 800 examples. Why
do that?</p>
<p>Well, labeling is costly:</p>
<ul>
<li>Benchmarks take time to run.</li>
<li>Some configurations require complex rebuilds.</li>
<li>Human evaluation is slow, expensive, and error-prone.</li>
</ul>
<p>Prior work has shown that even with big compute, building labeled
datasets takes <strong>months or years</strong>. EZR sidesteps this by
labeling only what matters most.</p>
<h3 id="so-how-does-ezr-help">So how does EZR help?</h3>
<p>EZR operates under a <strong>tiny-AI assumption</strong>: most of the
signal is buried under a layer of irrelevant or redundant data. It was
inspred byt George Kelly’s Personal Construct Theory (from the 1950s)
that stressed the value of contrast set learning to recognize what is
different between concepts.</p>
<p>Modern versions of contrast set learning include various rule-based
learners including our own previous work on TAR3 and WHICH. EZR’s
variant on contrast set learning throws away most of that architecture
and repalces it with a simple two-class classification approach. Tables
of data describing the “best” and “rest” examples can compute the
likilihoods <em>b,r</em> of of an as-yet-unclassified example belonging
to “best” or “rest”. The next example to label is the one that maximizes
some <em>acquisiton function</em> e.g. <em>b/r</em>.</p>
<h3 id="what-is-new-about-ezr">What is New About EZR?</h3>
<p>This work builds upon the foundation laid by prior resarch, but it
removes some key barriers that limits the practical use of thsose
methods.</p>
<p>Often we are asked “if EZR just’ simplifies existing methods, it is
worthy of publciation?”. If so many past resarechers have explored
simplifty (see Table 1), what is new and itneresting about EZR?</p>
<p>In reply, we say offer the observation that propr simplicity results
are alomst uneirsally ignored. Each year we interview dozens of
propsectivegraduate students for our reserch labs. We also interact,
extensively, with industrial pracitioners and many other researchers at
conferences. Nearly unviersally, when faced with any new AI problem,
thei default action is to reach for some alrge language model or deep
learning solution.</p>
<p>simplification research is valid resaerch. A persistant human
congintive basi is to under-value subtractive impormvents. Klotz et
al. report nmueris studies where</p>
<p>case studis where hmans are given patterns on a chess baord, then
asked to change the pattern in order to make it more symmetrical.
OVerhwlemmning, humans prefer to add new squares to the pattern, rather
than take squares away.</p>
<p>As Klotz et al. warns:</p>
<blockquote>
<p>“Defaulting to searches for additive changes may be one reason that
people struggle to mitigate overburdened schedules, institutional red
tape and damaging effects on the planet.”</p>
</blockquote>
<h3 id="why-use-a-decision-tree">Why use a decision tree?</h3>
<p>Decision trees are:</p>
<ul>
<li><strong>Fast to train</strong><br />
</li>
<li><strong>Sparse by nature</strong><br />
</li>
<li><strong>Easy to read</strong></li>
</ul>
<p>As physicist Ernest Rutherford famously quipped:<br />
&gt; “A theory that you can’t explain to a bartender is probably no damn
good.”</p>
<p>Each path in the tree offers a <strong>human-readable recipe for
improvement</strong>, grounded in real measurements.</p>
<h3 id="how-is-win-defined">How is “win” defined?</h3>
<p>Each row has <em>n</em> goals that we are trying to minimize or
maximize. In the labelled data, each goal column has a min and max value
seen so far. After normalizng each goal has the value 0..1 (for
min..max), we can say that minimizing goals have a best value of “0” and
maximizing goals have a best value of “1”.</p>
<pre><code>y(row) = sqrt(sum(abs(norm(goal) - goal.most)^2 for goal in cols.y))</code></pre>
<p>We normalize utility as:</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>win <span class="op">=</span> <span class="dv">100</span> × (<span class="dv">1</span> <span class="op">-</span> (mean(y) <span class="op">-</span> best) <span class="op">/</span> (median <span class="op">-</span> best))</span></code></pre></div>
<ul>
<li>A win of <strong>100</strong> means we match the best.<br />
</li>
<li>A win of <strong>0</strong> means we’re at the median.<br />
</li>
<li>Negative scores mean we’ve regressed.</li>
</ul>
<hr />
<h3 id="how-general-is-this">How General Is This?</h3>
<p>The database example is just one of over 100 benchmarks in the <a
href="https://github.com/timm/moot">MOOT</a> repository. Each MOOT
dataset has: - Between 1,000 and 100,000 rows - 5 to 1,000+
configuration choices - Up to 3 goal metrics</p>
<p>XXXX need to explain EZR</p>
<p>We tested EZR 10× on each dataset. In each trial: - <em>A = 4</em>,
<em>B = 24</em>, <em>C = 5</em> - EZR built a model from training data -
Test data was ranked by the model - Top 5 predictions were compared to
ground truth - We recorded the “win” of the best predicted row</p>
<p>As a baseline, we also tested <strong>dumb guessing</strong>: picking
5 random test rows, and scoring the best one.</p>
<h4 id="result-percentiles-of-win-scores">Result (Percentiles of Win
Scores)</h4>
<p>The results were sorted and divided into percentils (top 10%, next
10%, etc):</p>
<table>
<thead>
<tr>
<th style="text-align: right;">Percentile</th>
<th style="text-align: right;">EZR</th>
<th style="text-align: left;">Bar Chart(of EZR)</th>
<th style="text-align: right;">EZR - Dumb</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">100</td>
<td style="text-align: right;">100</td>
<td style="text-align: left;">*************************</td>
<td style="text-align: right;">148</td>
</tr>
<tr>
<td style="text-align: right;">90</td>
<td style="text-align: right;">100</td>
<td style="text-align: left;">*************************</td>
<td style="text-align: right;">64</td>
</tr>
<tr>
<td style="text-align: right;">80</td>
<td style="text-align: right;">99</td>
<td style="text-align: left;">*************************</td>
<td style="text-align: right;">43</td>
</tr>
<tr>
<td style="text-align: right;">70</td>
<td style="text-align: right;">93</td>
<td style="text-align: left;">************************</td>
<td style="text-align: right;">27</td>
</tr>
<tr>
<td style="text-align: right;">60</td>
<td style="text-align: right;">81</td>
<td style="text-align: left;">*********************</td>
<td style="text-align: right;">15</td>
</tr>
<tr>
<td style="text-align: right;">50</td>
<td style="text-align: right;">70</td>
<td style="text-align: left;">*******************</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: right;">40</td>
<td style="text-align: right;">59</td>
<td style="text-align: left;">****************</td>
<td style="text-align: right;">3</td>
</tr>
<tr>
<td style="text-align: right;">30</td>
<td style="text-align: right;">42</td>
<td style="text-align: left;">*************</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: right;">20</td>
<td style="text-align: right;">35</td>
<td style="text-align: left;">************</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: right;">10</td>
<td style="text-align: right;">17</td>
<td style="text-align: left;">******</td>
<td style="text-align: right;">-12</td>
</tr>
</tbody>
</table>
<p>Note that, one time in ten, even dumb guessing gets lucky and does
surprisingly well (the 10% case where “dumb” is 12% better than EZR).
But we do not want to be “dumb” since dumb reasoning does not
generalize. On the other hand, <strong>EZR’s trees provide both
performance and understanding.</strong> To say that another way, dumb
reasonng just says yes or now. EZR tells you how and why.</p>
<p>ALso, while dumb guessing is simple, EZR is barely more complex—just
a few hundred lines of code—and runs fast. The full experiment (10× on
110 datasets) took just <strong>65 seconds</strong> on a Mac mini with
no GPU.</p>
<h3 id="and-what-does-all-this-tell-us">And What Does All This Tell
Us?</h3>
<p>That sometimes, <strong>complexity is unnecessary</strong>.</p>
<p>With small tools, small data, and smart strategies, we can solve
real-world optimization tasks—effectively, quickly, and
transparently.</p>
<p>EZR demonstrates how to teach and practice software engineering
grounded in:</p>
<ul>
<li><strong>Simplicity</strong></li>
<li><strong>Critique</strong></li>
<li><strong>Ownership</strong></li>
</ul>
<p>—all without sacrificing performance.</p>
<h2 id="data">Data</h2>
<p>To understand the code, first we have to understand the data it
processess. Here’s a typical input file. In this example, the goal is to
to tune Spout wait, Spliters, Counters in order to maximize Throughput
(marked with a
<code>+'') and minimze Latency (marked with a</code>-’’).</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> independent values          <span class="op">|</span> y <span class="op">=</span> dependent values</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="op">--------------------------------|----------------------</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  Spout_wait, Spliters, Counters, <span class="op">|</span> Throughput<span class="op">+</span>, Latency<span class="op">-</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>     <span class="dv">10</span>,        <span class="dv">6</span>,       <span class="dv">17</span>,      <span class="op">|</span>    <span class="dv">23075</span>,    <span class="fl">158.68</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>      <span class="dv">8</span>,        <span class="dv">6</span>,       <span class="dv">17</span>,      <span class="op">|</span>    <span class="dv">22887</span>,    <span class="fl">172.74</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>      <span class="dv">9</span>,        <span class="dv">6</span>,       <span class="dv">17</span>,      <span class="op">|</span>    <span class="dv">22799</span>,    <span class="fl">156.83</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>      <span class="dv">9</span>,        <span class="dv">3</span>,       <span class="dv">17</span>,      <span class="op">|</span>    <span class="dv">22430</span>,    <span class="fl">160.14</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    ...,      ...,      ...,           ...,    ...</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>  <span class="dv">10000</span>,        <span class="dv">1</span>,       <span class="dv">10</span>,      <span class="op">|</span>   <span class="fl">460.81</span>,    <span class="fl">8761.6</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>  <span class="dv">10000</span>,        <span class="dv">1</span>,       <span class="dv">18</span>,      <span class="op">|</span>   <span class="fl">402.53</span>,    <span class="fl">8797.5</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>  <span class="dv">10000</span>,        <span class="dv">1</span>,       <span class="dv">12</span>,      <span class="op">|</span>   <span class="fl">365.07</span>,    <span class="fl">9098.9</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>  <span class="dv">10000</span>,        <span class="dv">1</span>,        <span class="dv">1</span>,      <span class="op">|</span>   <span class="fl">310.06</span>,    <span class="dv">9421</span></span></code></pre></div>
<p>EZR processes tables of data with x inputs and y goals.</p>
<ul>
<li>The first row shows the column names.</li>
<li>Numeric columns start with uppercase, all others are /symbolic.</li>
<li>Goal columns (e.g. Fig. 3’s Throughput+, Latency-) use +/- to denote
maximize and minimize.</li>
</ul>
<p>For the purposes of illustration, the rows in this table are sorted
from best to worst based on those goals (during experimentation, row
order should initially be randomized),</p>
<p>For the purposes of evaluation, all rows in MOOT data sets contain
all their y values. When evaluating the outcome of an optimizer, these
values are used to determine how well the optimizer found the best
rows.</p>
<p>For the purposes of optimization experiments, researchers should hide
the y-values from the optimizer. Each time the optimizer requests the
value of a particular row, this “costs” one unit. Good optimizers find
good goals at least cost (i.e. fewest labels).</p>
<h2 id="structs">Structs</h2>
<p>EZR does not use Python classes since this lets use spread
functionalty of structs over multiple files (this is useful since, it
lets place arcane detail out of the way of the simpler stuff). Instead,
its structs are based on <code>SimpleNamespace</code> objects that can
pretty print themselves, and which allow access to content via the
standard dot notation.</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> types <span class="im">import</span> SimpleNamespace <span class="im">as</span> o</span></code></pre></div>
<p>In EZR, rows of data are summarized into
<code></code>Num<code>s (numeric) or</code>Sym<code>(symbolic) columns. Each column knows the</code>txt<code>of its name  and its column position (in</code>at<code>).</code>Sym<code>s keep a count of the symbols seen so far (in</code>has`).</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Sym(at<span class="op">=</span><span class="dv">0</span>, txt<span class="op">=</span><span class="st">&quot;&quot;</span>): </span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> o(it<span class="op">=</span>Sym, at<span class="op">=</span>at, txt<span class="op">=</span>txt, has<span class="op">=</span>{})</span></code></pre></div>
<p><code>Num</code>s also keep track of the smallest and largest number
seen so far (<code>lo</code>, <code>hi</code>) as well as the mean
<code>mu</code> and standard devaition <code>sd</code>. Further,
<code>Num</code>s also know if they are a goal that needs minimization
(if the last letter of its name is <code>-</code>, then <em>better</em>
values are <em>smaller</em>).</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Num(at<span class="op">=</span><span class="dv">0</span>, txt<span class="op">=</span><span class="st">&quot; &quot;</span>): </span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> o(it<span class="op">=</span>Num, at<span class="op">=</span>at, txt<span class="op">=</span>txt, lo<span class="op">=</span><span class="fl">1e32</span>, mu<span class="op">=</span><span class="dv">0</span>, m2<span class="op">=</span><span class="dv">0</span>, sd<span class="op">=</span><span class="dv">0</span>, n<span class="op">=</span><span class="dv">0</span>, </span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>           hi<span class="op">=-</span><span class="fl">1e32</span>, more <span class="op">=</span> <span class="dv">0</span> <span class="cf">if</span> txt[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> <span class="st">&quot;-&quot;</span> <span class="cf">else</span> <span class="dv">1</span>)</span></code></pre></div>
<p><code>Num</code>s and <code>Syms</code> are generated from the names
on row1 of the data. Upper case names generated <code>Num</code>s and
other names are <code>Sym</code>s. Columns have different roles; e.g. if
their name ends in <code>+-</code> then they are part of the dependent
output variables <code>y</code>. Otherwise, they are part of the
<code>x</code> indepdentent inputs. Apart from that, they are also
present in the list of <code>all</code> columns.</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a> <span class="kw">def</span> Cols(names):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>   <span class="bu">all</span>, x, y, klass <span class="op">=</span> [],[],[],<span class="va">None</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>   <span class="cf">for</span> c,s <span class="kw">in</span> <span class="bu">enumerate</span>(names):</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>     <span class="bu">all</span> <span class="op">+=</span> [(Num <span class="cf">if</span> s[<span class="dv">0</span>].isupper() <span class="cf">else</span> Sym)(c,s)]</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>     <span class="cf">if</span> s[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> <span class="st">&quot;X&quot;</span>: <span class="cf">continue</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>     <span class="cf">if</span> s[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> <span class="st">&quot;!&quot;</span>: klass <span class="op">=</span> <span class="bu">all</span>[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>     (y <span class="cf">if</span> s[<span class="op">-</span><span class="dv">1</span>] <span class="kw">in</span> <span class="st">&quot;!-+&quot;</span> <span class="cf">else</span> x).append(<span class="bu">all</span>[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>   <span class="cf">return</span> o(it<span class="op">=</span>Cols, names<span class="op">=</span>names, <span class="bu">all</span><span class="op">=</span><span class="bu">all</span>, x<span class="op">=</span>x, y<span class="op">=</span>y, klass<span class="op">=</span>klass)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a> </span></code></pre></div>
<p>(Note if a column name ends in <code>X</code> it should be skiiped by
the inference. <code>Cols</code> implments this by skipping over such
columns and not adding them to the <code>x</code> or <code>y</code>
roles.)</p>
<p>All these columns, and their associated ros are stored within a
<code>Data</code>. This is just a set of <code>rows</code> and their
sumamries in <code>cols</code>. The first item in a input
<code>src</code> of information is grabbed and used by <code>COls</code>
to create a list of <code>cols</code> (columns). Any remaining items
within that src are then added to the <code>Data</code> struct.</p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Data(src):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  src <span class="op">=</span> <span class="bu">iter</span>(src)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> adds(src, o(it<span class="op">=</span>Data, n<span class="op">=</span><span class="dv">0</span>, rows<span class="op">=</span>[], cols<span class="op">=</span> Cols(<span class="bu">next</span>(src))))</span></code></pre></div>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adds(src, it<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>   <span class="co">&quot;A handy helper function for adding many values to some struct.&quot;</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>  it <span class="op">=</span> it <span class="kw">or</span> Num()</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  [add(it, x) <span class="cf">for</span> x <span class="kw">in</span> src]</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> it</span></code></pre></div>
<p>import random, math, sys, re</p>
<p>def atom(s): for fn in [int,float]:</p>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>: <span class="cf">return</span> fn(s)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> _: <span class="cf">pass</span></span></code></pre></div>
<p>s = s.strip() return {‘True’:True,‘False’:False}.get(s,s)</p>
<p>the = o(**{k:atom(v) for k,v in
re.findall(r”(+)=(+)“,<strong>doc</strong>)})</p>
<h2 id="code">Code</h2>
<p>The core of EZR is around 200 lines of Python. Around that core are
hundreds of lines of other code containing a test suite, some
statistical analysis, and a whole bunch of other methods that are not
essential or recommmended, but inclde here for comparisons purposes.</p>
<p>The code inputs a csv file</p>
<p>When reading that file, it is best to start at the top and bottom.
The bottom of the code contains a long ist of <code>eg__xxx</code>
functions which can be called from the command line
<code>python3 ezr.py --xxx</code>. To get a list of these functions, use
the <code>--list</code> flag. The functions are listed top to bottom,
simplest to most complex.</p>
<pre><code>&gt; ezr --list

ezr.py [OPTIONS]

   --list        List all examples
   --the         Show the system config.
   --sym         Sym: demo.
   --Sym         Sym: demo of likelihood.
   --num         Num: demo.
   --Num         Num: demo of likelihood.
   --data        Data: demo of reading from file.
   --inc         Data: updates. Incremental adds and deletes.
   --bayes       Like: find likelihoods for all rows.
   --stats       Stats: numeric (effect size and significance tests).
   --sk20        Stats: Checking rankings of 20 treatmeents, for increaseing sd
   --confuse     Stats: discrete calcs for recall, precision etc..
   --diabetes    Naive Bayes classifier: test on diabetes.
   --soybean     Naive Bayes classifier: test on soybean.
   --distx       Dist: check x distance calcs.
   --disty       Dist: check y distance calcs.
   --irisKpp     Dist: check Kmeans++ centroids on iris.
   --irisK       Dist: check Kmeans on iris.
   --fmap        Dist:  diversity-based optimziation with fast map.
   --rand        Dist:  diversity-based optimziation with random point selection.
   --c           ABC: compare tree and bayes for exploring test data.
   --tree        ABC: Test the tree on test data.
   --klass       ABC: acquire using &#39;klass&#39;.
   --old         ABC: compare diversity acquisition strategies.
   --liking      ABC: compare Bayesian acquisition strategies.
   --final       ABC: ompare the better acquisition strategies.</code></pre>
<p>The top of the code’s <code>__docstring__</code> shows the code’s
options, the most important of which are the <code>-A</code>,
<code>-B</code>, <code>-C</code> flags that control Any, Build, and
Check. The rest of the flags do thigs like set the random numer seed
(<code>-s</code>) and where to find input data (<code>-f</code>).</p>
<p>Python nodes - meta-model. Doc strings in files and functions.
globals (to get list of functions).</p>
<h1 id="easer-ai-why">Easer AI: Why?</h1>
<p>This section offers motivation for exploring little AI tools like
EZR.</p>
<h2 id="config-is-a-problem">Config is a problem</h2>
<p>asdas</p>
<h2 id="config-is-a-very-geenratl-problem.">#Config is a very geenratl
problem.</h2>
<p>HBO and icsmn ’24</p>
<h3 id="learning-about-ai">Learning About AI</h3>
<p>If we can make AI simpler, then we can make also simplify the
teaching of AI.</p>
<p>EZR is an interesting candidate for study, for the following
reasons:</p>
<ul>
<li>its system requirements are so low, it can run on system that are
already available to all of us;</li>
<li>it is compact and accessible;</li>
<li>it provides an extensive set of very usable facilities;</li>
<li>it is intrinsically interesting, and in fact breaks new ground in a
number of areas.</li>
</ul>
<p>Not least amongst the charms and virtues of EZR is the compactness of
its source code: in just a few hundred ines of code including tools for
clustering, classification, regression, optimization, explanation,
active learning, statistical analysis, documentation, and test-driven
development.</p>
<p>Such a short code listing is important. For <strong>industrial
practitioners:</strong>, short code examples are easier to understand,
adapt, test, maintain and (if required), port to different languages.
Another reason to explore short code solutions are the security
implications associated with building systems based on long supply
chains. To say the least, it is prudent to replace long supply chains
with tiny local stubs.</p>
<p>Also, for <strong>teaching (or self-study)</strong>, it has often
been suggested that 1,000 lines of code represents the practical limit
in size for a program which is to be understood and maintained by a
single individual<a href="#fn10" class="footnote-ref" id="fnref10"
role="doc-noteref"><sup>10</sup></a>. Most AI tools either exceed this
limit by two orders of magnitude, or else offer the user a very limited
set of facilities, i.e. either the details of the system are
inaccessible to all but the most determined, dedicated and
long-suffering student, or else the system is rather specialised and of
little intrinsic interest.</p>
<p>In my view, it is highly beneficial for anyone studying SE, AI, or
computer science to have the opportunity to study a working AI tool in
all its aspects. Moreover it is undoubtedly good for students majoring
in Computer Science, to be confronted at least once in their careers,
with the task of reading and understanding a program of major
dimensions.</p>
<p>It is my hope that this doc will be of interest and value to students
and practitioners of AI. Although not prepared primarily for use as a
reference work, some will wish to use it as such. For those people, this
code comes with extensive digressions on how parts of it illustrate
various aspects of SE, AI, or computer science.</p>
<h2 id="coding-style">Coding Style</h2>
<h3 id="no-oo">No OO</h3>
<p>No OO. hatton.</p>
<h3 id="dry">DRY</h3>
<p>docu</p>
<h3 id="tdd">TDD</h3>
<h3 id="min-loc.-keep-readability">Min LOC. Keep readability</h3>
<h4 id="functional">Functional</h4>
<h4 id="ternary">Ternary</h4>
<h4 id="auto-typing">Auto-typing</h4>
<h4 id="comprehensions">Comprehensions</h4>
<h3 id="dsl">DSL</h3>
<p>Rule of three</p>
<p>Accordingly, EZR.py usies active learnng to build models froma very
small amunt of dat. Its work can be sumamrised as A-B-C.</p>
<ul>
<li><strong>A</strong>: Use <strong>a</strong>ny examples</li>
<li><strong>B</strong>: <strong>B</strong>uild a model</li>
<li><strong>C</strong>: <strong>C</strong>heck the model</li>
</ul>
<p>EZR supports not just the code but allso the statsitical functions
that lets analst make clear concluios about (e.g.) what kinds of
clustering leads to better conclusions, sooner. With this it…</p>
<p>Teaching . illustrates much of what is missing in current programmer
and sE ltierature (oatterns of productinve coding, isuess of
documentation, encapultion test drivend evelopment etc). It can also be
used a minimal AI teaching toolkit that indotruces students to
clustering. Bayes inference, classfication, rule earling, tree elarning
as well as the stats required to devalauted which of these tools is best
for some current data/</p>
<h2 id="motivation">Motivation</h2>
<h3 id="should-make-it-simpler">Should make it simpler</h3>
<h3 id="can-make-i-simpler">Can make i simpler</h3>
<p>EZR was motivated by the current industrial obsession on Big AI that
seems to be forgetting centuries of experience with data mining. As far
back as 1901, Pearson<a href="#fn11" class="footnote-ref" id="fnref11"
role="doc-noteref"><sup>11</sup></a> showed that tables of data with
<span class="math inline"><em>N</em></span> columns can be modeled with
far fewer columns (where the latter are derived from the eigenvectors of
a correlation information).</p>
<p>Decades of subsequent work has shown that effective models can be
built from data that cover tiny fractions of the possible data space<a
href="#fn12" class="footnote-ref" id="fnref12"
role="doc-noteref"><sup>12</sup></a>. Levnina and Biclet cwnote that</p>
<blockquote>
<p>“The only reason any (learning) methods work … is that, in fact, the
data are not truly high-dimensional. Rather, they are .. can be
efficiently summarized in a space of a much lower dimension.</p>
</blockquote>
<p>(This remarks echoes an early conclusion from Johnson and
Lindenstrauss <a href="#fn13" class="footnote-ref" id="fnref13"
role="doc-noteref"><sup>13</sup></a>.).</p>
<p>For example:</p>
<ul>
<li><strong>Many rows can be ignored</strong>: Data sets with thousands
of rows can be modeled with just a few dozen samples<a href="#fn14"
class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>.
To explain this, suppose we only want to use models that are well
supported by the data; i.e. supported by multiple rows in a table of
data. This means that many rows in a table can be be replaced by a
smaller number of exemplars.</li>
<li><strong>Many columns can be ignored</strong>: High-dimensional
tables (with many colummns) can be projected into lower dimensional
tables while nearly preserving all pairwise distances<a href="#fn15"
class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>.
This means that data sets with many columns can be modeled with
surprisingly few columns. e.g. A table of (say) of <span
class="math inline"><em>C</em> = 20</span> columns of binary variables
have a total data space of <span
class="math inline">2<sup>20</sup></span> (which is more than a
million). Yet with just dozens to hundred rows of training data, it is
often possible to build predictors from test rows from that data space.
This is only possible if the signal in this data condenses to a small
regions within the total data space.</li>
<li>Researchers in semi-supervised learning note that high-dimensional
data often lies on a simpler, lower-dimensional ”manifold” embedded
within that higher space <a href="#fn16" class="footnote-ref"
id="fnref16" role="doc-noteref"><sup>16</sup></a>.</li>
</ul>
<p>Numerous AI researchers studying NP-hard tasks report the existence
of a small number of key variables that determine the behavior of the
rest of the model. When such keys are present, then the problem of
controlling an entire model simplifies to just the problem of
controlling the keys.</p>
<p>Keys have been discovered in AI many times and called many different
names: Variable subset selection, narrows, master variables, and
backdoors. In the 1960s, Amarel observed that search problems contain
narrows; i.e. tiny sets of variable settings that must be used in any
solution<a href="#fn17" class="footnote-ref" id="fnref17"
role="doc-noteref"><sup>17</sup></a>. Amarel’s work defined macros that
encode paths between the narrows in the search space, effectively
permitting a search engine to leap quickly from one narrow to
another.</p>
<p>In later work, data mining researchers in the 1990s explored and
examined what happens when a data miner deliberately ignores some of the
variables in the training data. Kohavi and John report trials of data
sets where up to 80% of the variables can be ignored without degrading
classification accuracy<a href="#fn18" class="footnote-ref" id="fnref18"
role="doc-noteref"><sup>18</sup></a>. Note the similarity with Amarel’s
work: it is more important to reason about a small set of important
variables than about all the variables. At the same time, researchers in
constraint satisfaction found “random search with retries” was a very
effective strategy.</p>
<p>Crawford and Baker reported that such searches took less time than a
complete search to find more solutions using just a small number of
retries<a href="#fn19" class="footnote-ref" id="fnref19"
role="doc-noteref"><sup>19</sup></a>. Their ISAMP “iterative sampler”
makes random choices within a model until it gets “stuck”; i.e. until
further choices do not satisfy expectations. When “stuck”, ISAMP does
not waste time fiddling with current choices (as was done by older
chronological backtracking algorithms). Instead, ISAMP logs what
decisions were made before getting “stuck”. It then performs a “retry”;
i.e. resets and starts again, this time making other random choices to
explore.</p>
<p>Crawford and Baker explain the success of this strange approach by
assuming models contain a small set of master variables that set the
remaining variables (and this paper calls such master variables keys).
Rigorously searching through all variable settings is not recommended
when master variables are present, since only a small number of those
settings actually matter. Further, when the master variables are spread
thinly over the entire model, it makes no sense to carefully explore all
parts of the model since much time will be wasted “walking” between the
far-flung master variables. For such models, if the reasoning gets stuck
in one region, then the best thing to do is to leap at random to some
distant part of the model.</p>
<p>A similar conclusion comes from the work of Williams et al.<a
href="#fn20" class="footnote-ref" id="fnref20"
role="doc-noteref"><sup>20</sup></a>. They found that if a randomized
search is repeated many times, that a small number of variable settings
were shared by all solutions. They also found that if they set those
variables before conducting the rest of the search, then formerly
exponential runtimes collapsed to low-order polynomial time. They called
these shared variables the backdoor to reducing computational
complexity.</p>
<p>Combining the above, we propose the following strategy for faster
reasoning about RE models. First, use random search with retries to find
the “key” decisions in RE models. Second, have stakeholders debate, and
then decide, about the keys before exploring anything else. Third, to
avoid trivially small solutions, our random search should strive to
cover much of the model. Code:</p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Data(src):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> _guess(row):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sum</span>(interpolate(data,row,<span class="op">*</span>pole) <span class="cf">for</span> pole <span class="kw">in</span> poles)<span class="op">/</span><span class="bu">len</span>(poles)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>  head, <span class="op">*</span>rows <span class="op">=</span> <span class="bu">list</span>(src)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>  data  <span class="op">=</span> _data(head, rows)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>  poles <span class="op">=</span> projections(data)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> row <span class="kw">in</span> rows: row[<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> _guess(row)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> data</span></code></pre></div>
<h3 id="data-1">Data</h3>
<p>Shared datasets from research papers by Apel [2], Chen <a
href="#fn21" class="footnote-ref" id="fnref21"
role="doc-noteref"><sup>21</sup></a>, and Menzies [^nair] are often used
as case studies of optimization in SE research papers. Chen and Menzies
are collaborating to curate the MOOT repository (Multi-Objective
Optimization Testing4) which offers datasets from recent SE optimization
papers for process tuning, DB configuration, HPO, management decision
making etc.</p>
<p>Since our focus is on configuration, we use MOOT data related to that
task (see Table I and II). Fig. 3 shows the typical structure of those
MOOT data sets. The goal in this data is to tune Spout wait, Spliters,
Counters in order to achieve the best Throughput/Latency. In
summary:</p>
<ul>
<li>MOOT datasets are tables with x inputs and y goals.</li>
<li>The first row shows the column names.</li>
<li>Numeric columns start with uppercase, all others are /symbolic.</li>
<li>Goal columns (e.g. Fig. 3’s Throughput+, Latency-) use +/- to denote
maximize and minimize.</li>
</ul>
<p>Data:</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> independent values          <span class="op">|</span> y <span class="op">=</span> dependent values</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>  <span class="op">--------------------------------|----------------------</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>  Spout_wait, Spliters, Counters, <span class="op">|</span> Throughput<span class="op">+</span>, Latency<span class="op">-</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>     <span class="dv">10</span>,        <span class="dv">6</span>,       <span class="dv">17</span>,      <span class="op">|</span>    <span class="dv">23075</span>,    <span class="fl">158.68</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>      <span class="dv">8</span>,        <span class="dv">6</span>,       <span class="dv">17</span>,      <span class="op">|</span>    <span class="dv">22887</span>,    <span class="fl">172.74</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>      <span class="dv">9</span>,        <span class="dv">6</span>,       <span class="dv">17</span>,      <span class="op">|</span>    <span class="dv">22799</span>,    <span class="fl">156.83</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>      <span class="dv">9</span>,        <span class="dv">3</span>,       <span class="dv">17</span>,      <span class="op">|</span>    <span class="dv">22430</span>,    <span class="fl">160.14</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    ...,      ...,      ...,           ...,    ...</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>  <span class="dv">10000</span>,        <span class="dv">1</span>,       <span class="dv">10</span>,      <span class="op">|</span>   <span class="fl">460.81</span>,    <span class="fl">8761.6</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>  <span class="dv">10000</span>,        <span class="dv">1</span>,       <span class="dv">18</span>,      <span class="op">|</span>   <span class="fl">402.53</span>,    <span class="fl">8797.5</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>  <span class="dv">10000</span>,        <span class="dv">1</span>,       <span class="dv">12</span>,      <span class="op">|</span>   <span class="fl">365.07</span>,    <span class="fl">9098.9</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>  <span class="dv">10000</span>,        <span class="dv">1</span>,        <span class="dv">1</span>,      <span class="op">|</span>   <span class="fl">310.06</span>,    <span class="dv">9421</span></span></code></pre></div>
<p>Note that our data is much larger than the Table 3 example. The 39
data sets in Table I have up to 86,000 rows, 88 independent variables,
and three y goals. For the purposes of illustration, the rows in Table 3
are sorted from best to worst based on those goals. During
experimentation, row order should initially be randomized.</p>
<p>For the purposes of evaluation, all rows in MOOT data sets contain
all their y values. When evaluating the outcome of an optimizer, these
values are used to determine how well the optimizer found the best
rows.</p>
<p>For the purposes of optimization experiments, researchers should hide
the y-values from the optimizer. Each time the optimizer requests the
value of a particular row, this “costs” one unit. For reasons described
below, good optimizers find good goals at least cost (i.e. fewest
labels).</p>
<p>Notes from ase aper</p>
<h2 id="rq5-lmits">RQ5: Lmits</h2>
<p>not generation.</p>
<p>Tabular data</p>
<h2 id="references">References</h2>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>S. Amarel, “Program synthesis as a theory formation
task: problem representations and solution methods,” in Machine
Learning: An Artificial Intelligence Approach. Morgan Kaufmann, 1986.<a
href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Chang, C. L. (1974). Finding Prototypes for Nearest
Neighbor Classifiers. IEEE Transactions on Computers, C-23(11),
1179–1184.–<a href="#fnref2" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>W. B. Johnson and J. Lindenstrauss, “Extensions of
lipschitz mappings into a hilbert space,” Contemporary Mathematics,
vol. 26, pp. 189–206, 1984.<a href="#fnref3" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>J. M. Crawford and A. B. Baker, “Experimental results on
the application of satisfiability algorithms to scheduling problems,” in
Proceedings of the Twelfth National Conference on Artificial
Intelligence (Vol. 2), Menlo Park, CA, USA, 1994, pp. 1092–1097.<a
href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>R. Kohavi and G. H. John, “Wrappers for feature subset
selection,” Artif. Intell., vol. 97, no. 1-2, pp. 273–324, Dec. 1997.<a
href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>R. Williams, C. P. Gomes, and B. Selman, “Backdoors to
typical case complexity,” in Proceedings of the International Joint
Conference on Artificial Intelligence, 2003.<a href="#fnref6"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>D. Zhou et al., “Learning with Local and Global
Consistency”, 2005.<a href="#fnref7" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>T. Menzies, “The Few Key Rows”, 2008 (or your actual
source).<a href="#fnref8" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>B. Settles, “Active Learning Literature Survey”, 2009.<a
href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Lions, John (1996). Lions’ Commentary on UNIX 6th
Edition with Source Code. Peer-to-Peer Communications. ISBN
978-1-57398-013-5.<a href="#fnref10" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>Pearson, K. (1901). “On Lines and Planes of Closest Fit
to Systems of Points in Space”. Philosophical Magazine. 2 (11): 559–572.
10.1080/14786440109462720.<a href="#fnref11" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>I. Witten, E. Frank, and M. Hall. Data Mining:
Practical Machine Learning Tools and Techniques Morgan Kaufmann Series
in Data Management Systems Morgan Kaufmann, Amsterdam, 3 edition,
(2011)<a href="#fnref12" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>W. B. Johnson and J. Lindenstrauss, “Extensions of
lipschitz mappings into a hilbert space,” Contemporary Mathematics,
vol. 26, pp. 189–206, 1984.<a href="#fnref13" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>T. Menzies, “The Few Key Rows”, 2008 (or your actual
source).<a href="#fnref14" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>W. B. Johnson and J. Lindenstrauss, “Extensions of
lipschitz mappings into a hilbert space,” Contemporary Mathematics,
vol. 26, pp. 189–206, 1984.<a href="#fnref15" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>D. Zhou et al., “Learning with Local and Global
Consistency”, 2005.<a href="#fnref16" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>S. Amarel, “Program synthesis as a theory formation
task: problem representations and solution methods,” in Machine
Learning: An Artificial Intelligence Approach. Morgan Kaufmann, 1986.<a
href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p>R. Kohavi and G. H. John, “Wrappers for feature subset
selection,” Artif. Intell., vol. 97, no. 1-2, pp. 273–324, Dec. 1997.<a
href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p>J. M. Crawford and A. B. Baker, “Experimental results
on the application of satisfiability algorithms to scheduling problems,”
in Proceedings of the Twelfth National Conference on Artificial
Intelligence (Vol. 2), Menlo Park, CA, USA, 1994, pp. 1092–1097.<a
href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p>R. Williams, C. P. Gomes, and B. Selman, “Backdoors to
typical case complexity,” in Proceedings of the International Joint
Conference on Artificial Intelligence, 2003.<a href="#fnref20"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p>M. Li, T. Chen, and X. Yao, “How to evaluate solutions
in pareto-based search-based software engineering: A critical review and
methodological guidance,” IEEE Transactions on Software Engineering,
vol. 48, no. 5, pp. 1771–1799, 2022<a href="#fnref21"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
